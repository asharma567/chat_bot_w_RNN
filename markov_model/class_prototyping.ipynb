{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- finish grid search \\n\\n\\n- add dedupe by datamade \\n- think about graph based methods\\n- lsi\\nhttp://blog.josephwilk.net/projects/latent-semantic-analysis-in-python.html\\nhttps://radimrehurek.com/gensim/dist_lsi.html\\n- add the DBSCAN bit\\n- lookup norwigs thing about how properly design a class\\n- lsh blog\\nhttp://matthewcasperson.blogspot.com/2013/11/minhash-for-dummies.html\\nhttps://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-minhash.html\\nhttp://stackoverflow.com/questions/19701052/how-many-hash-functions-are-required-in-a-minhash-algorithm/19711615#19711615\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- finish grid search \n",
    "\n",
    "\n",
    "- add dedupe by datamade \n",
    "- think about graph based methods\n",
    "- lsi\n",
    "http://blog.josephwilk.net/projects/latent-semantic-analysis-in-python.html\n",
    "https://radimrehurek.com/gensim/dist_lsi.html\n",
    "- add the DBSCAN bit\n",
    "- lookup norwigs thing about how properly design a class\n",
    "- lsh blog\n",
    "http://matthewcasperson.blogspot.com/2013/11/minhash-for-dummies.html\n",
    "https://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-minhash.html\n",
    "http://stackoverflow.com/questions/19701052/how-many-hash-functions-are-required-in-a-minhash-algorithm/19711615#19711615\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NearestNeighbors(algorithm='brute', leaf_size=30, metric='minkowski',\n",
      "         metric_params=None, n_jobs=4, n_neighbors=5, p=2, radius=1.0)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "[joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using \"if __name__ == '__main__'\". Please see the joblib documentation on Parallel for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-b462fc26b929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mdeduper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeduper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mdeduper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mhisto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeduper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_hist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-b462fc26b929>\u001b[0m in \u001b[0;36mmake_hist\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajay/anaconda/lib/python2.7/site-packages/sklearn/neighbors/base.pyc\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 dist = pairwise_distances(X, self._fit_X, 'euclidean',\n\u001b[0;32m--> 371\u001b[0;31m                                           n_jobs=n_jobs, squared=True)\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 dist = pairwise_distances(\n",
      "\u001b[0;32m/Users/ajay/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajay/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     ret = Parallel(n_jobs=n_jobs, verbose=0)(\n\u001b[1;32m   1059\u001b[0m         \u001b[0mfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         for s in gen_even_slices(Y.shape[0], n_jobs))\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajay/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_effective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajay/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_initialize_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0malready_forked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0malready_forked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                     raise ImportError('[joblib] Attempting to do parallel computing '\n\u001b[0m\u001b[1;32m    516\u001b[0m                             \u001b[0;34m'without protecting your import on a system that does '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                             \u001b[0;34m'not support forking. To use parallel-computing in a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: [joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using \"if __name__ == '__main__'\". Please see the joblib documentation on Parallel for more information"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "# need to add a class for graph theory\n",
    "# nn class\n",
    "class Deduper_NN(object):\n",
    "    '''\n",
    "    I need to re-evaluate whether or not I want the state of the model/vector space \n",
    "    being saved in the event that I dont' I should just kill the self.model = model.fit() stuff\n",
    "    and pass parameters from one function to another.\n",
    "    \n",
    "    methods\n",
    "    ------\n",
    "    train\n",
    "        - model type\n",
    "    predict\n",
    "    preprocess\n",
    "        - various stuff\n",
    "    '''\n",
    "\n",
    "    metrics = [\n",
    "        'cosine', \n",
    "        'euclidean',\n",
    "        'dice', \n",
    "        'jaccard', \n",
    "        'braycurtis',\n",
    "        'canberra', \n",
    "    ]\n",
    "    \n",
    "    vector_space = None\n",
    "\n",
    "    def read_in_the_file(self, file_name):\n",
    "        \n",
    "        #read in subject file\n",
    "        with open(file_name) as f:\n",
    "             self.orig_file = [line.strip() for line in f]\n",
    "    \n",
    "    def build_vectorizer(self, corpus, model_type='bag of words', ngrams=1, tokenizer='char'):\n",
    "        '''\n",
    "        *add word2vec\n",
    "        '''\n",
    "        \n",
    "        #think of params\n",
    "        params = {\n",
    "            'analyzer': tokenizer,\n",
    "            'ngram_range' : (1, ngrams)\n",
    "        }\n",
    "    \n",
    "        if model_type == 'bag of words':\n",
    "            vectorizer = CountVectorizer(**params)\n",
    "        elif model_type ==  'tfidf':\n",
    "            vectorizer = TfidfVectorizer(**params)\n",
    "        \n",
    "        self.vector_space = vectorizer.fit_transform(corpus) \n",
    "        self.vectorizer = vectorizer \n",
    "    \n",
    "    def find_all_duplicates(self):\n",
    "        \n",
    "        #find all duplicates\n",
    "        all_dups_dict = {idx : self.predict(line) for idx, line in enumerate(self.orig_file)}\n",
    "        return all_dups_dict\n",
    "    \n",
    "    def fit_model(self, model_type='brute', vector_space_params={}):\n",
    "        '''\n",
    "        fits model operating under the assumption that there's a model already built\n",
    "        '''\n",
    "\n",
    "        if model_type == 'brute':\n",
    "            self.model = NearestNeighbors(algorithm='brute')\n",
    "        elif model_type == 'lsh':\n",
    "            self.model = LSHForest()\n",
    "        elif model_type == 'annoy':\n",
    "            self.model = Annoy()\n",
    "\n",
    "        self.model.fit(self.vector_space)\n",
    "        print self.model        \n",
    "\n",
    "    def predict(self, new_data_pt, radius_threshold=.25):\n",
    "        '''\n",
    "        not sure how to find the optimal threshold here\n",
    "        '''\n",
    "        #careful to note that it takes a single string and converts to a list object of strings\n",
    "        pt = self.vectorizer.transform([new_data_pt])\n",
    "        \n",
    "        #how to find optimal radius?\n",
    "        distance_from_origin, indices = self.model.radius_neighbors(pt, radius=radius_threshold)\n",
    "        \n",
    "        #unpacking\n",
    "        distance_from_origin = distance_from_origin[0]\n",
    "        indices = indices[0]\n",
    "\n",
    "        grabbing_the_lines_from_file = [self.orig_file[index] for index in indices]\n",
    "\n",
    "        return grabbing_the_lines_from_file\n",
    "    \n",
    "    def grid_search(self):\n",
    "        '''\n",
    "        I: target string\n",
    "        O: prints all combinations of comparisons\n",
    "        \n",
    "        * this goes in the master deduper class\n",
    "        '''\n",
    "        \n",
    "        #preprocessing variables\n",
    "            #spaces or no spaces\n",
    "            #combinations there of.\n",
    "        \n",
    "        vector_space_params = {\n",
    "            'model_types' : ['bag of words', 'tfidf'], #add lsi and word2vec\n",
    "            'ngrams' : [1,2,3,4],\n",
    "            'tokenizer' : ['char', 'word'],\n",
    "            #or some combination there of, to do this we need to output and concat\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        #fit the vector-space\n",
    "        #char-grams, words\n",
    "            #unagrams, bigrams, tri-grams\n",
    "            \n",
    "        \n",
    "        #model selection\n",
    "        model_params = {\n",
    "            'model_type' : ['brute', 'annoy', 'lsh']\n",
    "            #fill the rest in later\n",
    "        }\n",
    "        \n",
    "        #distances\n",
    "        metrics = [\n",
    "            'cosine', \n",
    "            'euclidean',\n",
    "            'dice', \n",
    "            'jaccard', \n",
    "            'braycurtis',\n",
    "            'canberra', \n",
    "        ]\n",
    "        \n",
    "        model_params['metrics'] = metric\n",
    "        \n",
    "        all_params = {\n",
    "            'preprocessing': None,\n",
    "            'vector_space': vector_space_params,\n",
    "            'nn_algo': model_params,\n",
    "        }\n",
    "        \n",
    "        for nn_algo in all_params['nn_algo']['model_type']:\n",
    "            for vector_space_model in all_params['vector_space']['model_types']:\n",
    "                for gram in  all_params['vector_space']['ngrams']:\n",
    "                    for type_of_tokenizer in  all_params['vector_space']['tokenizer']:\n",
    "                        for metric in metrics:\n",
    "                            params = {\n",
    "                                nn_algo,\n",
    "                                vector_space_model,\n",
    "                                type_of_tokenizer,\n",
    "                                metric\n",
    "                            }\n",
    "                            deduper.train()\n",
    "                            #print to out \n",
    "                \n",
    "        \n",
    "        #how do you gauge the quality of matches?\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    #since this isn't a nn search model it belongs in the biggest deduper\n",
    "    def brute_force_deduper(self, list_of_strings, comparison_algo, threshold=None):\n",
    "        '''\n",
    "        I: self explanatory\n",
    "        O: dictionary {string: sorted list of matches}\n",
    "        '''\n",
    "        big_bag = {}\n",
    "        #to deep copy or not to deep copy\n",
    "\n",
    "        for index, s1 in enumerate(list_of_strings):\n",
    "            small_bag = get_all_comparisons(list_of_strings[index:], comparison_algo)\n",
    "            big_bag[s1] = sorted(small_bag, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        return big_bag\n",
    "    \n",
    "    def make_hist(self):\n",
    "        #use a numpy array since the size is already pre-defined\n",
    "        hist_bag = []\n",
    "        \n",
    "        for index in self.vector_space.indices:\n",
    "\n",
    "            observation = self.vector_space[index]\n",
    "\n",
    "            dist, idx = self.model.kneighbors(observation, n_neighbors=2)\n",
    "            dist, idx = dist[0], idx[0]\n",
    "\n",
    "            #find out which position the current index is in\n",
    "            remove_this_arg = [k for k, i in enumerate(idx) if i == index]\n",
    "            dist = [i for k, i in enumerate(dist) if i != remove_this_arg[0]]\n",
    "            \n",
    "            hist_bag.append(dist)\n",
    "                \n",
    "        return hist_bag\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    deduper = Deduper_NN()\n",
    "    deduper.read_in_the_file('target.txt')\n",
    "    deduper.build_vectorizer(deduper.orig_file)\n",
    "    deduper.fit_model()\n",
    "    histo = deduper.make_hist()\n",
    "\n",
    "    for i in range(10):\n",
    "        print '=' * 50\n",
    "        print '\\n'.join(deduper.predict(deduper.orig_file[i], 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "[joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using \"if __name__ == '__main__'\". Please see the joblib documentation on Parallel for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-748f966d3fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeduper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeduper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajay/anaconda/lib/python2.7/site-packages/sklearn/neighbors/base.pyc\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 dist = pairwise_distances(X, self._fit_X, 'euclidean',\n\u001b[0;32m--> 371\u001b[0;31m                                           n_jobs=n_jobs, squared=True)\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 dist = pairwise_distances(\n",
      "\u001b[0;32m/Users/ajay/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajay/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     ret = Parallel(n_jobs=n_jobs, verbose=0)(\n\u001b[1;32m   1059\u001b[0m         \u001b[0mfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         for s in gen_even_slices(Y.shape[0], n_jobs))\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajay/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_effective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ajay/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_initialize_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0malready_forked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0malready_forked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                     raise ImportError('[joblib] Attempting to do parallel computing '\n\u001b[0m\u001b[1;32m    516\u001b[0m                             \u001b[0;34m'without protecting your import on a system that does '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                             \u001b[0;34m'not support forking. To use parallel-computing in a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: [joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using \"if __name__ == '__main__'\". Please see the joblib documentation on Parallel for more information"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import numpy as np\n",
    "    new_data_pt = 'have a great day gre'\n",
    "    #careful to note that it takes a single string and converts to a list object of strings\n",
    "\n",
    "    \n",
    "    hist_bag = []\n",
    "    for index in deduper.vector_space.indices:\n",
    "\n",
    "        observation = deduper.vector_space[index]\n",
    "\n",
    "        dist, idx = deduper.model.kneighbors(observation, n_neighbors=1)\n",
    "        dist, idx = dist[0], idx[0]\n",
    "\n",
    "#         #find out which position the current index is in\n",
    "#         remove_this_arg = [k for k, i in enumerate(idx) if i == index]\n",
    "#         dist = [i for k, i in enumerate(dist) if i != remove_this_arg[0]]\n",
    "\n",
    "#         hist_bag.append(dist)\n",
    "\n",
    "        break\n",
    "\n",
    "    #instead of naively removing the first one lets just remove the one with the same index\n",
    "    \n",
    "    \n",
    "\n",
    "#how to find optimal radius?\n",
    "# distance_from_origin, indices = model.kneighbors(pt, radius=radius_threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histogram_maker(model):\n",
    "    # loop through all the points in the nearest neighbor algo\n",
    "    # return a histogram, prints time it took to loop through all the points\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(91, 'check1')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_comparisons(main_str, strings, comparison_algo, threshold=None):\n",
    "    '''\n",
    "    I: string, list of strings, string comparison algo eg. levenshtien, threshold\n",
    "    O: list of tuples (match score, weight) \n",
    "    \n",
    "    Takes a target string and compares it to the rest of strings in the list\n",
    "    USE --\n",
    "\n",
    "    get_all_comparisons('check', ['check1'], fuzz.ratio) \n",
    "    >>> [(91, 'check1')]\n",
    "    '''\n",
    "    match_bag = []\n",
    "\n",
    "    for str_ in strings:\n",
    "        match_rating = comparison_algo(main_str, str_)\n",
    "\n",
    "        if threshold:\n",
    "            if match_rating > threshold:\n",
    "                match_bag.append((match_rating, str_))\n",
    "        else:\n",
    "            match_bag.append((match_rating, str_))\n",
    "\n",
    "    return match_bag\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "get_all_comparisons('check', ['check1'], fuzz.ratio) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
